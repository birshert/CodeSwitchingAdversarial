В своей работе мы выполнили все поставленные перед началом работы цели, а именно:

\begin{itemize}
    \item Обучили мультиязычные модели для задачи заполнения слотов и классификации интентов
    \item Провели две адверсариальные атаки на обученные модели и замерили качество
    \item Обучили модели по предложенному методу адверсариального предобучения и атаковали полученные модели
    \item Проанализировали полученные результаты
\end{itemize}

\parМы выяснили, что мультиязычные языковые модели m-BERT и XLM-RoBERTa могут хорошо справляться со смешением кодов в задаче заполнения слотов и классификации интентов.
Так же мы выяснили, что модели обученные на тренировочной выборке из семи языков более робастные, чем обученные на выборке на английском языке.

\parВ качестве дальнейшей работы можно рассматривать следующие направления:

\begin{itemize}
    \item Рассмотреть в экспериментах большее количество мультиязычных моделей
    \item Рассмотреть в экспериментах альтернативные варианты защиты от атак
    \item Распространить опыт экспериментов с адверсариальными атаками на другие области обработки естественного языка
\end{itemize}
