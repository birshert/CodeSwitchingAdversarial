\subsection{Мультиязычные модели}
Языки с небольшим количеством данных часто не могут предоставить достаточного размера датасета для обучения с учителем.
Существует подход для борьбы с этим, который заключается в построении кросс-язычных представлений.
Эти представления нужно дообучать для специфичной задачи на языке с большим количеством ресурсов, чтобы показывать хорошее качество на других, менее ресурсоёмких языках~\cite{klementiev-etal-2012-inducing}.
\parВслед за успехом модели Трансформер~\cite{Vaswani2017AttentionIA},
недавние мультиязычные модели такие как m-BERT~\cite{devlin-etal-2019-bert} и XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR}
переносят парадигму <<предобучение $\rightarrow$ дообучение под специфическую задачу>> в мультиязычную область.
Они предобучают энкодеры на основе архитектуры Трансформера на текстовых данных с различными задачами языкового моделирования.
Затем эти предобученные энкодеры могут быть дообучены для конкретной задачи на ресурсоёмком языке для которого есть много размеченных данных.
Это известно как кросс-язычный перенос знаний.
\parВ одних недавних исследованиях кросс-язычного переноса знаний было показано, что качество модели на ранее не виденных тестовых языках сильно зависит от количества обучающих данных и размера контекста~\cite{Liu2020WhatMM}.
В~\cite{Wu2019BetoBB} было показано, что m-BERT показывает очень сильную способность к кросс-язычному переносу знаний.
m-BERT превосходит по качеству мультиязычные эмбеддинги в четырёх из пяти исследуемых задач без какой-либо информации о связи языков.
\par Более современная и более сложная модель XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR} показывает лучшее, чем m-BERT качество, однако требует массивных объемов обучающих данных для хорошей работы.
В своём исследовании авторы XLM-RoBERTa показывают, что их модель является самой сильной мультиязычной моделью на текущий момент.
\par m-BERT обучается на корпусах Wikipedia и Books, в то время как XLM-RoBERTa обучается на CommonCrawl, который содержит для многих языков на несколько порядков больше данных.

\subsection{Классификация интентов и заполнение слотов}
Повсеместное использование виртуальных ассистентов постепенно становится ежедневной реальностью с ростом их популярности.
Богатство возможностей и качество работы ассистента напрямую влияет на удобство его использования.
Хорошие ассистенты будут привлекать всё больше людей, занимая доли рынка.
Ключевым аспектом в работе виртуального помощника является правильная классификация интентов и заполнение слотов в запросах.
Интент — это желаемый результат запроса пользователя.
Слоты — это слова или наборы слов, которые содержат релевантную интенту информацию.
\parИз-за тесной корреляции между задачами заполнения слотов и классификации интентов обычно используется одна модель для одновременного решения обеих задач~\cite{Weld2021ASO}.
Актуальные подходы последнего времени используют модели на основе Трансформера, например BERT~\cite{devlin-etal-2019-bert}.
Одним из популярных датасетов для этой задачи является датасет MultiAtis++~\cite{Xu2020EndtoEndSA}.

\subsection{Смешение кодов в адверсариальных атаках на мультиязычные модели}
Первые статьи в тематике адверсариальных атак с использованием смешения кодов на мультиязычные модели, такие как m-BERT~\cite{devlin-etal-2019-bert}, Unicoder~\cite{huang-etal-2019-unicoder} и XLM-RoBERTa~\cite{Conneau2020UnsupervisedCR} вышли в середине апреля 2021 года.
В своей работе мы опирались и рассматривали две такие статьи - ~\cite{Tan2021CodeMixingOS} и~\cite{Krishnan2021MultilingualCF}.
\parПервое исследование~\cite{Tan2021CodeMixingOS} посвящено анализу качества моделей m-BERT, Unicoder и XLM-RoBERTa на датасете XNLI под влиянием адверсариальных атак.
В своей работе авторы анализируют две адверсариальные атаки, одна word-level (замена слов в предложении на их эквиваленты из набора языков), вторая phrase-level (замена частей предложения с помощью построения выравниваний).
Так же авторы предлагают метод адверсариального предобучения, который заключается в генерации дополнительных адверсариальных пертурбаций для обучающей выборки и обучения на ней.
\parВторое исследование~\cite{Krishnan2021MultilingualCF} посвящено анализу метода аугментации данных для задачи заполнения слотов и классификации предложений.
В своей работе авторы ставят перед собой цель улучшить перенос знаний на новый неизвестный язык для мультиязычной языковой модели.
Аналогично первой статье, во втором исследовании анализ сконцентрирован на качестве моделей после аугментирования тренировочных данных с помощью смешения кодов.
Авторы предлагают метод chunk-level атаки, которая заключается в сегментации предложений по спанам слотов и замене соответствующих одинаковых сегментов между предложениями на разных языках.
С помощью такой атаки они атакуют обучающую выборку датасета MultiAtis++~\cite{Xu2020EndtoEndSA} и добавляют полученную адверсариальную выборку к исходной обучающей выборке.
\parВ первой статье было установлено, что проведенные адверсариальные атаки очень сильно ухудшили качество мультиязычных языковых моделей.
В первую очередь это может быть связано с тем, что датасет XNLI сам по себе является сложным датасетом и изначальное качество моделей не высоко.
С другой стороны стоит отметить, что в адверсариальных атаках из этого исследования использовалась схема <<один основной язык, много встраиваемых языков>>.
В то время как эта схема не может отражать реалистичные данные со смешением кодов, так как большинство людей, которые могут смешивать коды в речи или на письме, билингвы~\cite{bilinguals}.
\parВо второй статье было установлено, что добавление аугментированных данных в обучающую выборку приводит к улучшению качества для модели m-BERT\@.
Использование дополнительных данных положительно повлияло на низкоресурсные языки.
Так же это положительно сказалось на качестве на языках с отличными от английского морфологическими структурами — китайским и японским.

\subsection{Машинный перевод и выравнивание слов}
Для машинного перевода в своей работе мы будем использовать~\cite{Fan2020BeyondEM}.
Созданная авторами статьи модель обучалась на внушительном датасете из 7.5 миллиардов предложений для 100 языков.
Данная модель основна на архитектуре Трансформера и способна переводить с любого на любой язык в пределе ста обучающих.
На текущий момент это одна из самых сильных моделей для машинного перевода, которая успешно справляется с переводом на любые, даже ранее низкоресурсные, языки.
\parДля построения выравниваний между параллельными предложениями на разных языках мы будем использовать~\cite{Dou2021WordAB}.
Оригинальный подход авторов статьи использует эмбеддинги от мультиязычной языковой модели m-BERT~\cite{devlin-etal-2019-bert}.
Среди результатов постулируется превосходство данного подхода над всеми остальными на текущий момент.
